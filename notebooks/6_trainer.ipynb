{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "from model import unet\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "from torchsummary import summary\n",
    "from utils.imgShow import imsShow\n",
    "from utils.dataloader import TraSet, ValSet\n",
    "from utils.acc_metric import oa_binary, miou_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_scene = 'data/dset/scene/'\n",
    "dir_dem = 'data/dset/dem/' \n",
    "dir_truth = 'data/dset/truth/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scenes: 48, vali patch: 1959\n"
     ]
    }
   ],
   "source": [
    "### secene and truth pairwise data\n",
    "## traset\n",
    "ids_tra_gdf = gpd.read_file('data/dset/dset_tra.gpkg')\n",
    "ids_tra = ids_tra_gdf['id_scene'].tolist()\n",
    "paths_scene_tra = [dir_scene+id+'_nor.tif' for id in ids_tra]\n",
    "paths_dem_tra = [dir_dem+id+'_dem_nor.tif' for id in ids_tra]\n",
    "paths_truth_tra = [dir_truth+id+'.tif' for id in ids_tra] \n",
    "## valset\n",
    "paths_patch_valset = sorted(glob('data/dset/valset/*'))\n",
    "print(f'train scenes: {len(paths_scene_tra)}, vali patch: {len(paths_patch_valset)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset instances\n",
    "tra_data = TraSet(paths_scene=paths_scene_tra, \n",
    "                   paths_truth=paths_truth_tra, \n",
    "                   paths_dem=paths_dem_tra,\n",
    "                   path_size=(256, 256))\n",
    "val_data = ValSet(paths_valset=paths_patch_valset)\n",
    "tra_loader = torch.utils.data.DataLoader(tra_data, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing time: 5.0\n",
      "Batch processing time: 6.7\n",
      "Batch processing time: 8.3\n",
      "Batch processing time: 13.2\n",
      "Batch processing time: 14.8\n",
      "Batch processing time: 16.5\n",
      "Batch processing time: 21.3\n",
      "Batch processing time: 26.5\n",
      "Batch processing time: 28.2\n",
      "Batch processing time: 29.8\n",
      "Batch processing time: 31.4\n",
      "Batch processing time: 33.0\n"
     ]
    }
   ],
   "source": [
    "## check data loading time\n",
    "time_start = time.time()\n",
    "for x_batch, y_batch in tra_loader:\n",
    "  print(f\"Batch processing time: {time.time() - time_start:.1f}\")\n",
    "time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Upsample: 1-1                          --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Conv2d: 2-1                       1,024\n",
      "|    └─BatchNorm2d: 2-2                  32\n",
      "|    └─ReLU: 2-3                         --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-4                       4,640\n",
      "|    └─BatchNorm2d: 2-5                  64\n",
      "|    └─ReLU: 2-6                         --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Conv2d: 2-7                       18,496\n",
      "|    └─BatchNorm2d: 2-8                  128\n",
      "|    └─ReLU: 2-9                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Conv2d: 2-10                      73,856\n",
      "|    └─BatchNorm2d: 2-11                 256\n",
      "|    └─ReLU: 2-12                        --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Conv2d: 2-13                      110,656\n",
      "|    └─BatchNorm2d: 2-14                 128\n",
      "|    └─ReLU: 2-15                        --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Conv2d: 2-16                      41,520\n",
      "|    └─BatchNorm2d: 2-17                 96\n",
      "|    └─ReLU: 2-18                        --\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─Conv2d: 2-19                      18,464\n",
      "|    └─BatchNorm2d: 2-20                 64\n",
      "|    └─ReLU: 2-21                        --\n",
      "├─Sequential: 1-9                        --\n",
      "|    └─Conv2d: 2-22                      289\n",
      "|    └─Sigmoid: 2-23                     --\n",
      "=================================================================\n",
      "Total params: 269,713\n",
      "Trainable params: 269,713\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Upsample: 1-1                          --\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─Conv2d: 2-1                       1,024\n",
       "|    └─BatchNorm2d: 2-2                  32\n",
       "|    └─ReLU: 2-3                         --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Conv2d: 2-4                       4,640\n",
       "|    └─BatchNorm2d: 2-5                  64\n",
       "|    └─ReLU: 2-6                         --\n",
       "├─Sequential: 1-4                        --\n",
       "|    └─Conv2d: 2-7                       18,496\n",
       "|    └─BatchNorm2d: 2-8                  128\n",
       "|    └─ReLU: 2-9                         --\n",
       "├─Sequential: 1-5                        --\n",
       "|    └─Conv2d: 2-10                      73,856\n",
       "|    └─BatchNorm2d: 2-11                 256\n",
       "|    └─ReLU: 2-12                        --\n",
       "├─Sequential: 1-6                        --\n",
       "|    └─Conv2d: 2-13                      110,656\n",
       "|    └─BatchNorm2d: 2-14                 128\n",
       "|    └─ReLU: 2-15                        --\n",
       "├─Sequential: 1-7                        --\n",
       "|    └─Conv2d: 2-16                      41,520\n",
       "|    └─BatchNorm2d: 2-17                 96\n",
       "|    └─ReLU: 2-18                        --\n",
       "├─Sequential: 1-8                        --\n",
       "|    └─Conv2d: 2-19                      18,464\n",
       "|    └─BatchNorm2d: 2-20                 64\n",
       "|    └─ReLU: 2-21                        --\n",
       "├─Sequential: 1-9                        --\n",
       "|    └─Conv2d: 2-22                      289\n",
       "|    └─Sigmoid: 2-23                     --\n",
       "=================================================================\n",
       "Total params: 269,713\n",
       "Trainable params: 269,713\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check model\n",
    "model = unet(num_bands=7)\n",
    "summary(model, input_size=(7,256,256), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create loss and optimizer\n",
    "loss_bce = nn.BCELoss()     \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "def train_step(model, loss_fn, optimizer, x, y):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------validation step------'''\n",
    "def val_step(model, loss_fn, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.float())\n",
    "        loss = loss_fn(pred, y.float())\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------train loops------'''\n",
    "def train_loops(model, loss_fn, optimizer, tra_loader, \n",
    "                                    val_loader, epoches, device):\n",
    "    model = model.to(device)\n",
    "    size_tra_loader = len(tra_loader)\n",
    "    size_val_loader = len(val_loader)\n",
    "    for epoch in range(epoches):\n",
    "        start = time.time()\n",
    "        tra_loss, val_loss = 0, 0\n",
    "        tra_miou, val_miou = 0, 0\n",
    "        tra_oa, val_oa = 0, 0\n",
    "        '''-----train the model-----'''\n",
    "        for x_batch, y_batch in tra_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = train_step(model=model, loss_fn=loss_fn, \n",
    "                                    optimizer=optimizer, x=x_batch, y=y_batch)\n",
    "            tra_loss += loss.item()\n",
    "            tra_miou += miou.item()\n",
    "            tra_oa += oa.item()\n",
    "        '''-----validation the model-----'''\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = val_step(model=model, loss_fn=loss_fn, \n",
    "                                                    x=x_batch, y=y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_miou += miou.item()\n",
    "            val_oa += oa.item()\n",
    "        ## Accuracy\n",
    "        tra_loss = tra_loss/size_tra_loader\n",
    "        val_loss = val_loss/size_val_loader\n",
    "        tra_miou = tra_miou/size_tra_loader\n",
    "        val_miou = val_miou/size_val_loader\n",
    "        tra_oa = tra_oa/size_tra_loader\n",
    "        val_oa = val_oa/size_val_loader\n",
    "        print(f'Ep{epoch+1}: tra-> Loss:{tra_loss:.3f},Oa:{tra_oa:.2f},Miou:{tra_miou:.2f}, '\n",
    "              f'val-> Loss:{val_loss:.2f},Oa:{val_oa:.2f},Miou:{val_miou:.2f},time:{time.time()-start:.0f}s')\n",
    "        ## show the result\n",
    "        if (epoch+1)%10 == 0:\n",
    "            model.eval()\n",
    "            sam_index = random.randrange(len(val_data))\n",
    "            patch, truth = val_data[sam_index]\n",
    "            patch, truth = torch.unsqueeze(patch.float(), 0).to(device), truth.to(device)\n",
    "            pred = model(patch)\n",
    "            ## convert to numpy and plot\n",
    "            patch = patch[0].to('cpu').detach().numpy().transpose(1,2,0)\n",
    "            pred = pred[0].to('cpu').detach().numpy()\n",
    "            truth = truth.to('cpu').detach().numpy()\n",
    "            imsShow([patch, truth, pred], \n",
    "                    img_name_list=['input_patch', 'truth', 'prediction'] , figsize=(10,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1: tra-> Loss:0.565,Oa:0.51,Miou:0.35, val-> Loss:0.33,Oa:0.70,Miou:0.54,time:38s\n",
      "Ep2: tra-> Loss:0.310,Oa:0.93,Miou:0.80, val-> Loss:0.34,Oa:0.76,Miou:0.65,time:38s\n",
      "Ep3: tra-> Loss:0.310,Oa:0.90,Miou:0.80, val-> Loss:0.18,Oa:0.80,Miou:0.72,time:38s\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cpu') \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loops(model=model, \n",
    "            loss_fn=loss_bce, \n",
    "            optimizer=optimizer,\n",
    "            tra_loader=tra_loader, \n",
    "            val_loader=val_loader, \n",
    "            epoches=20,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model saving\n",
    "# path_save = 'model/trained/unet.pth'\n",
    "# torch.save(model.state_dict(), path_save)   # save weights of the trained model \n",
    "# model.load_state_dict(torch.load(path_save, weights_only=True))  # load the weights of the trained model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
