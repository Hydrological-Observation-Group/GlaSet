{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "from model import unet\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from notebooks import config\n",
    "from torchsummary import summary\n",
    "from utils.imgShow import imsShow\n",
    "from utils.dataloader import TraSet, ValSet\n",
    "from utils.acc_metric import oa_binary, miou_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scenes: 48, vali patch: 1959\n"
     ]
    }
   ],
   "source": [
    "### secene and truth pairwise data\n",
    "paths_truth = sorted(glob('data/dset/truth/*.tif'))\n",
    "paths_scene = [path.replace('truth', 'scene').replace('.tif', '_nor.tif') for path in paths_truth]\n",
    "paths_dem = [path.replace('truth','dem').replace('.tif', '_dem_nor.tif') for path in paths_truth]\n",
    "## traset\n",
    "paths_truth_tra = [paths_truth[i] for i in config.scene_ids_tra]\n",
    "paths_scene_tra = [paths_scene[i] for i in config.scene_ids_tra]\n",
    "paths_dem_tra = [paths_dem[i] for i in config.scene_ids_tra]\n",
    "## valset\n",
    "paths_patch_valset = sorted(glob('data/dset/valset/*'))\n",
    "print(f'train scenes: {len(paths_scene_tra)}, vali patch: {len(paths_patch_valset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset instances\n",
    "tra_data = TraSet(paths_scene=paths_scene_tra, \n",
    "                   paths_truth=paths_truth_tra, \n",
    "                   paths_dem=paths_dem_tra,\n",
    "                   path_size=(256, 256))\n",
    "val_data = ValSet(paths_valset=paths_patch_valset)\n",
    "tra_loader = torch.utils.data.DataLoader(tra_data, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing time: 1.7\n",
      "Batch processing time: 6.8\n",
      "Batch processing time: 8.3\n",
      "Batch processing time: 9.9\n",
      "Batch processing time: 14.9\n",
      "Batch processing time: 16.4\n",
      "Batch processing time: 20.9\n",
      "Batch processing time: 22.5\n",
      "Batch processing time: 24.1\n",
      "Batch processing time: 25.6\n",
      "Batch processing time: 27.2\n",
      "Batch processing time: 32.1\n"
     ]
    }
   ],
   "source": [
    "## check data loading time\n",
    "time_start = time.time()\n",
    "for x_batch, y_batch in tra_loader:\n",
    "  print(f\"Batch processing time: {time.time() - time_start:.1f}\")\n",
    "time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]           1,024\n",
      "       BatchNorm2d-2         [-1, 16, 256, 256]              32\n",
      "              ReLU-3         [-1, 16, 256, 256]               0\n",
      "            Conv2d-4         [-1, 32, 128, 128]           4,640\n",
      "       BatchNorm2d-5         [-1, 32, 128, 128]              64\n",
      "              ReLU-6         [-1, 32, 128, 128]               0\n",
      "            Conv2d-7           [-1, 64, 64, 64]          18,496\n",
      "       BatchNorm2d-8           [-1, 64, 64, 64]             128\n",
      "              ReLU-9           [-1, 64, 64, 64]               0\n",
      "           Conv2d-10          [-1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-11          [-1, 128, 32, 32]             256\n",
      "             ReLU-12          [-1, 128, 32, 32]               0\n",
      "         Upsample-13          [-1, 128, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]         110,656\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "         Upsample-17           [-1, 64, 64, 64]               0\n",
      "           Conv2d-18           [-1, 48, 64, 64]          41,520\n",
      "      BatchNorm2d-19           [-1, 48, 64, 64]              96\n",
      "             ReLU-20           [-1, 48, 64, 64]               0\n",
      "         Upsample-21         [-1, 48, 128, 128]               0\n",
      "           Conv2d-22         [-1, 32, 128, 128]          18,464\n",
      "      BatchNorm2d-23         [-1, 32, 128, 128]              64\n",
      "             ReLU-24         [-1, 32, 128, 128]               0\n",
      "         Upsample-25         [-1, 32, 256, 256]               0\n",
      "           Conv2d-26          [-1, 1, 256, 256]             289\n",
      "          Sigmoid-27          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 269,713\n",
      "Trainable params: 269,713\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.75\n",
      "Forward/backward pass size (MB): 89.00\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 91.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### check model\n",
    "model = unet(num_bands=7)\n",
    "summary(model, input_size=(7,256,256), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create loss and optimizer\n",
    "loss_bce = nn.BCELoss()     \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "def train_step(model, loss_fn, optimizer, x, y):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------validation step------'''\n",
    "def val_step(model, loss_fn, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y.float())\n",
    "    miou = miou_binary(pred=pred, truth=y)\n",
    "    oa = oa_binary(pred=pred, truth=y)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------train loops------'''\n",
    "def train_loops(model, loss_fn, optimizer, tra_loader, \n",
    "                                    val_loader, epoches, device):\n",
    "    model = model.to(device)\n",
    "    size_tra_loader = len(tra_loader)\n",
    "    size_val_loader = len(val_loader)\n",
    "    for epoch in range(epoches):\n",
    "        start = time.time()\n",
    "        tra_loss, val_loss = 0, 0\n",
    "        tra_miou, val_miou = 0, 0\n",
    "        tra_oa, val_oa = 0, 0\n",
    "        '''-----train the model-----'''\n",
    "        for x_batch, y_batch in tra_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = train_step(model=model, loss_fn=loss_fn, \n",
    "                                    optimizer=optimizer, x=x_batch, y=y_batch)\n",
    "            tra_loss += loss.item()\n",
    "            tra_miou += miou.item()\n",
    "            tra_oa += oa.item()\n",
    "        '''-----validation the model-----'''\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = val_step(model=model, loss_fn=loss_fn, \n",
    "                                                    x=x_batch, y=y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_miou += miou.item()\n",
    "            val_oa += oa.item()\n",
    "        ## Accuracy\n",
    "        tra_loss = tra_loss/size_tra_loader\n",
    "        val_loss = val_loss/size_val_loader\n",
    "        tra_miou = tra_miou/size_tra_loader\n",
    "        val_miou = val_miou/size_val_loader\n",
    "        tra_oa = tra_oa/size_tra_loader\n",
    "        val_oa = val_oa/size_val_loader\n",
    "        print(f'Ep{epoch+1}: tra-> Loss:{tra_loss:.3f},Oa:{tra_oa:.2f},Miou:{tra_miou:.2f}, '\n",
    "              f'val-> Loss:{val_loss:.2f},Oa:{val_oa:.2f},Miou:{val_miou:.2f},time:{time.time()-start:.0f}s')\n",
    "        ## show the result\n",
    "        if (epoch+1)%10 == 0:\n",
    "            model.eval()\n",
    "            sam_index = random.randrange(len(val_data))\n",
    "            patch, truth = val_data[sam_index]\n",
    "            patch, truth = torch.unsqueeze(patch, 0).to(device), truth.to(device)\n",
    "            pred = model(patch)\n",
    "            ## convert to numpy and plot\n",
    "            patch = patch[0].to('cpu').detach().numpy().transpose(1,2,0)\n",
    "            pred = pred[0].to('cpu').detach().numpy()\n",
    "            truth = truth.to('cpu').detach().numpy()\n",
    "            imsShow([patch, truth, pred], \n",
    "                    img_name_list=['input_patch', 'truth', 'prediction'] , figsize=(10,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1: tra-> Loss:0.463,Oa:0.79,Miou:0.68, val-> Loss:0.53,Oa:0.76,Miou:0.65,time:37s\n",
      "Ep2: tra-> Loss:0.309,Oa:0.83,Miou:0.70, val-> Loss:0.19,Oa:0.79,Miou:0.71,time:37s\n",
      "Ep3: tra-> Loss:0.114,Oa:0.95,Miou:0.86, val-> Loss:0.19,Oa:0.80,Miou:0.72,time:37s\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cpu') \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loops(model=model, \n",
    "            loss_fn=loss_bce, \n",
    "            optimizer=optimizer,\n",
    "            tra_loader=tra_loader, \n",
    "            val_loader=val_loader, \n",
    "            epoches=20,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model saving\n",
    "# path_save = 'model/trained/unet_l5789_s2.pth'\n",
    "# torch.save(model.state_dict(), path_save)   # save weights of the trained model \n",
    "# model.load_state_dict(torch.load(path_save, weights_only=True))  # load the weights of the trained model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gla-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
